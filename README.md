# PySpark Analysis Project

Проект для аналізу, очищення та машинного навчання на даних LendingClub з використанням Apache Spark.

## Використання

**Запуск контейнера:**

```bash
# Запустити контейнер (перший раз)
docker-compose up --build -d

# Запустити контейнер (наступні рази)
docker-compose up -d
```

**Етап 1: Обробка даних (ETL)**
Перед початком аналізу необхідно завантажити та очистити дані:

```bash
docker exec -it pyspark_app python3 -m src.dataset.process
```

**Етап 2: Бізнес-аналітика**
Виконання SQL-запитів для отримання бізнес-інсайтів:

```bash
docker exec -it pyspark_app python3 -m src.main
```

**Етап 3: Машинне навчання (ML)**
Запуск тренування моделей та оцінки результатів:

```bash
# Запустити повний цикл (всі задачі та моделі)
docker exec -it pyspark_app python3 -m src.ml.runner

# Запустити тільки задачу регресії
docker exec -it pyspark_app python3 -m src.ml.runner --task regression

# Запустити тільки конкретну модель (наприклад, GBT)
docker exec -it pyspark_app python3 -m src.ml.runner --model gbt
```

**Зупинка:**

```bash
docker-compose down
```

## Структура

```
├── data/                          # Дані (монтується в /app/data)
│   ├── big_data.csv               # Вхідний датасет
│   ├── cleaned-data/              # Spark part-файли
│   ├── cleaned_data_merged.csv    # Об'єднаний очищений датасет для ML
│   ├── models/                    # Збережені навчені моделі
│   ├── results/                   # CSV звіти бізнес-питань
│   └── visualizations/            # Збережені графіки (розподіл даних та ML)
├── src/
│   ├── main.py                    # Головна точка входу (бізнес-питання)
│   ├── dataset/                   # Модуль обробки даних
│   │   ├── spark_session.py       # Ініціалізація Spark
│   │   ├── loader.py              # Завантаження даних
│   │   ├── cleaner.py             # Очищення даних
│   │   ├── visualizer.py          # Візуалізація даних
│   │   ├── exporter.py            # Експорт даних
│   │   ├── merger.py              # Об'єднання part-файлів
│   │   └── process.py             # Головний пайплайн ETL
│   ├── ml/                        # Модуль Машинного Навчання
│   │   ├── preprocessor.py        # Векторизація, скейлінг, імп'ютація
│   │   ├── trainer.py             # Навчання моделей (RF, GBT, LR)
│   │   ├── visualizer.py          # Генерація графіків порівняння моделей
│   │   └── runner.py              # Точка входу ML
│   ├── questions/                 # Бізнес-логіка (SQL запити)
│   └── utils/                     # Допоміжні функції
├── docker-compose.yml
├── Dockerfile
└── pyproject.toml                 # Залежності проекту
```

## Як працює обробка даних

Пайплайн обробки даних складається з кількох етапів:

### 1. Завантаження даних
Датасет LendingClub автоматично завантажується з Google Drive при першому запуску. Файл зберігається локально в `data/big_data.csv`, тому при повторних запусках завантаження не відбувається.

### 2. Очищення даних
Це основний етап, де ми приводимо "сирі" дані до нормального вигляду:

- **Видалення зайвих колонок** — прибираємо `id`, `member_id`, `grade` та інші, які не потрібні для аналізу
- **Видалення колонок з пропусками** — якщо в колонці більше 30% значень відсутні, вона видаляється
- **Заповнення пропусків** — числові колонки заповнюються середнім значенням
- **Обробка FICO скорів** — рядки без FICO скорів видаляються (це важливі дані)
- **Перетворення колонки `term`** — прибираємо " months" і конвертуємо в число
- **One-Hot Encoding для `home_ownership`** — категоріальну змінну розбиваємо на окремі колонки
- **Кодування прапорців** — `hardship_flag` та `debt_settlement_flag` перетворюємо з Y/N в 1/0

### 3. Візуалізація даних
Після очищення генеруються графіки для аналізу розподілу даних:

**Гістограми основних показників** — показують як розподілені суми кредитів, процентні ставки та щомісячні платежі. Можна побачити, які суми кредитів найпопулярніші та який діапазон ставок переважає.

![Гістограми основних показників](visualizations/histograms.png)

**Боксплоти основних показників** — допомагають виявити викиди (аномальні значення) та зрозуміти медіанні значення. Якщо бачимо багато точок за межами "вусів" — це потенційні викиди.

![Боксплоти основних показників](visualizations/boxplots.png)

**Гістограми підмножини колонок** — загальний огляд розподілу перших 18 числових колонок датасету. Дає швидке уявлення про характер даних в цілому.

![Гістограми підмножини колонок](visualizations/subset_histograms.png)

### 4. Експорт
Очищені дані зберігаються у двох форматах:
- `data/cleaned-data/` — Spark part-файли (для подальшої роботи зі Spark)
- `data/cleaned_data_merged.csv` — єдиний CSV файл (зручно для перегляду та аналізу)

## Бізнес-аналітика

Модуль відповідає за виконання 12 комплексних SQL-запитів до даних для отримання бізнес-інсайтів. Використовується "сирий" набір даних для збереження повноти інформації (включаючи дати та текстові описи).

Основні напрямки аналізу:
- Аналіз прострочень та дефолтів у розрізі штатів США.
- Дослідження впливу мети кредиту (наприклад, "small_business" vs "wedding") на ймовірність повернення коштів.
- Порівняльний аналіз індивідуальних та спільних (Joint) заявок.
- Залежність процентної ставки від довжини кредитної історії.

Результати зберігаються у CSV файлах у директорії `data/results/`.

## Машинне навчання (ML)

Модуль реалізовано з використанням бібліотеки **Spark MLlib**. Він включає повний цикл: підготовку даних, навчання моделей, оцінку та візуалізацію результатів.

### Підготовка даних (Preprocessing)
- **Safe Casting:** Використання безпечного приведення типів (`try_cast`) для обробки забруднених даних CSV.
- **Imputation:** Заповнення пропусків середніми значеннями за допомогою `Imputer`.
- **VectorAssembler:** Збір усіх ознак у єдиний вектор.
- **StandardScaler:** Нормалізація даних (приведення до нульового середнього та одиничної дисперсії), що є критичним для лінійних моделей.
- **Feature Selection:** Використано розширений набір фінансових ознак (включаючи `revol_util`, `bc_open_to_buy`, `mort_acc`), що дозволило значно підвищити точність моделей.

### Задачі

**1. Класифікація (Risk Analysis)**
*   **Мета:** Передбачити дефолт позичальника.
*   **Цільова змінна:** `loan_status` (0 - кредит виплачено, 1 - дефолт).
*   **Моделі:** Logistic Regression, Random Forest Classifier, Gradient Boosted Trees (GBT).
*   **Метрики:** Accuracy, Precision, Recall, F1-Score.

**2. Регресія (Credit Scoring)**
*   **Мета:** Передбачити кредитний рейтинг FICO.
*   **Цільова змінна:** `fico_range_low` (числове значення).
*   **Моделі:** Linear Regression, Random Forest Regressor, Gradient Boosted Trees (GBT).
*   **Метрики:** RMSE (Root Mean Squared Error), R² (Coefficient of Determination).

### Результати
Після виконання ML пайплайну генеруються:
1.  **Збережені моделі:** У папці `data/models/`.
2.  **Графіки порівняння:** Порівняння метрик якості різних моделей у `data/visualizations/`.
3.  **Графіки важливості ознак:** Топ факторів, що впливають на прийняття рішення моделлю (Feature Importance).
```